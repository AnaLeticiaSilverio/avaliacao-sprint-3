{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python394jvsc74a57bd0c8a6a6b835292192219758eca8e0bb3a07a87f0ecc8435318658ce028efeb9f1",
      "display_name": "Python 3.9.4 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "orig_nbformat": 2,
    "metadata": {
      "interpreter": {
        "hash": "c8a6a6b835292192219758eca8e0bb3a07a87f0ecc8435318658ce028efeb9f1"
      }
    },
    "colab": {
      "name": "index.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "nXroWuW_Z5A4",
        "JGsD7Apt5oFs"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z-y7AXVZ5Ar"
      },
      "source": [
        "### Avaliação Sprint 3\n",
        "### Extrair o texto de 10 artigos jornalísticos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tr4pf0zZ5At"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "from pymongo import MongoClient\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.manifold import TSNE\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from time import time\n",
        "from spacy import displacy\n",
        "import pymongo\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAg4Exr409dw"
      },
      "source": [
        "### Importe SPACY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbn8tE2w08A4"
      },
      "source": [
        "#!python -m spacy download pt_core_news_sm\n",
        "nlp = spacy.load(\"pt_core_news_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeMpkrzlZ5Au"
      },
      "source": [
        "### Lendo o conteúdo das url"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umT98rSTZ5Au"
      },
      "source": [
        "url = open('/content/sample_data/noticias.txt','r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faV7dxruZ5Au"
      },
      "source": [
        "### Recebe os dados da resposta da url\n",
        "### Separa Títulos e Paragrafos\n",
        "### Soup para colegar dados Scrapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p82L9f8Z5Av"
      },
      "source": [
        "lista = []\n",
        "titulos = []\n",
        "links = []\n",
        "\n",
        "for noticia in url:\n",
        "\n",
        "    links.append(noticia)\n",
        "    response = urlopen(noticia)\n",
        "\n",
        "    html = response.read().decode('utf-8')\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    conteudo_titulo = soup.find('h1', class_=\"c-content-head__title\").getText()\n",
        "    titulos.append(conteudo_titulo)\n",
        "\n",
        "    conteudo_paragrafo = soup.find('div', class_=\"c-news__body\")\n",
        "    infos = conteudo_paragrafo.findAll('p')\n",
        "    lista.append(infos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8aaTGw6Z5Av"
      },
      "source": [
        "### Títulos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKcnlrX8zU7d"
      },
      "source": [
        "converte_titulos = []\n",
        "\n",
        "for titulo in titulos:\n",
        "  converte_titulos.append(titulo.strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2CgZ1wRzY_P"
      },
      "source": [
        "converte_titulos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_APOnWlZ5Av"
      },
      "source": [
        "converte_paragrafos = []\n",
        "\n",
        "for titulo in lista:\n",
        "  converte_paragrafos.append(titulo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_eZz6pJf75q"
      },
      "source": [
        "converte_paragrafos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c10ut1TftnzC"
      },
      "source": [
        "### Gerando Wordcloud Tratados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRMNz26EwNTC"
      },
      "source": [
        "len(converte_paragrafos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw2xCrRPxDMF"
      },
      "source": [
        "converte_paragrafos[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El8Hwl8txEou"
      },
      "source": [
        "converte_paragrafos[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6JzNG3sxHbS"
      },
      "source": [
        "for i in range(len(converte_paragrafos)):\n",
        "    interando = converte_paragrafos[i]\n",
        "    paragrafos_npl = nlp(str(interando))\n",
        "\n",
        "    tokens = [token for token in paragrafos_npl if not token.is_stop and token.is_punct != True]\n",
        "    todas_palavras_tweets = ' '.join([str(item) for item in tokens])\n",
        "    todas_palavras_tweets = todas_palavras_tweets.replace('< p >','').replace('</p >','')\n",
        "    logo = np.array(Image.open(path.join(\"/content/sample_data/logo.jpg\")))\n",
        "\n",
        "    stopwords = set(STOPWORDS)\n",
        "    stopwords.add(\"said\")\n",
        "\n",
        "    nuvem_palavras_tweets = WordCloud(width=1000, height=600, mask=logo,\n",
        "                            max_font_size=110, collocations=False, contour_color='gray',contour_width=3,background_color='white').generate(todas_palavras_tweets)\n",
        "\n",
        "    nuvem_palavras_tweets.generate(todas_palavras_tweets)\n",
        "\n",
        "    plt.figure(figsize=(20,10))\n",
        "    plt.imshow(nuvem_palavras_tweets, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    print(converte_titulos[i])\n",
        "    print(links[i])\n",
        " \n",
        "    plt.show()\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-iHktzNZ5Aw"
      },
      "source": [
        "### Paragrafos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ4Vc_51Z5Aw"
      },
      "source": [
        "paragrafos = []\n",
        "for i in lista:\n",
        "    paragrafos += i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50pkNZjEYrgt"
      },
      "source": [
        "paragrafos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4xANT_wZ5Ax"
      },
      "source": [
        "### Criando DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-g63Kg5Z5Ax"
      },
      "source": [
        "df = pd.DataFrame(paragrafos)\n",
        "df = df.rename(columns={0: 'Paragrafo'})\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebbruofdZ5Ax"
      },
      "source": [
        "### Conectando ao servidor\n",
        "### Conectando ao banco\n",
        "### Criando coleção\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDQUSVhYzmQg"
      },
      "source": [
        "#!pip install dnspython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYyNjt4DZ5Ax"
      },
      "source": [
        "cliente = pymongo.MongoClient(\"mongodb+srv://admin:admin@cluster0.rrlac.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\")\n",
        "banco = cliente['Noticias']  # buscando o nome do banco\n",
        "gravando = banco[\"paragrafos\"]  # buscando a tebale do banco"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2RmP7BmED82"
      },
      "source": [
        "### Transformando em uma to disc para grava no Mongo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se8wYLr7EGX_"
      },
      "source": [
        "dados = df.to_dict('list')\n",
        "valor = ((dados))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siJW5FWaEIMx"
      },
      "source": [
        "if not gravando.find_one(valor):       \n",
        "  gravando_banco = gravando.insert_one(valor)\n",
        "  print(f\"Gravado com sucesso: - {valor}\")\n",
        "else:\n",
        "    print(f'Já Existe')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDKIZ0IDZ5Ax"
      },
      "source": [
        "### Retornando todos os dados pelo Paragrafo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HujEdvtZ5Ay"
      },
      "source": [
        "for x in gravando.find({},{ \"_id\": 0, \"Paragrafo\": 1}):\n",
        "      retorno_do_banco = x\n",
        "retorno_do_banco"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0Fh3bOhZ5Ay"
      },
      "source": [
        "### Criando novo Dataset com base do dados vindo do mongodb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9xp8fiiFMpS"
      },
      "source": [
        "# Treinando Notícias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQtI5rjDZ5Ay"
      },
      "source": [
        "pd.set_option('display.max_column',None)\n",
        "pd.set_option('display.max_rows',None)\n",
        "pd.set_option('display.max_seq_items',None)\n",
        "pd.set_option('display.max_colwidth', 500)\n",
        "pd.set_option('expand_frame_repr', True)\n",
        "\n",
        "dataset = pd.DataFrame(retorno_do_banco)\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7lq01O-Z5Az"
      },
      "source": [
        "doc = nlp(str(dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTBbzvyDZ5Az"
      },
      "source": [
        "### Entidades"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iIH0kZWZ5Az"
      },
      "source": [
        "doc.ents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_EkSmEVZ5Az"
      },
      "source": [
        "### Tratando texto para minúsculo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvjyGFM4Z5Az"
      },
      "source": [
        "def trata_textos(doc):\n",
        "    tokens_validos = []\n",
        "    for token in doc:\n",
        "        e_valido = not token.is_stop and token.is_alpha\n",
        "        if e_valido:\n",
        "            tokens_validos.append(token.text)\n",
        "\n",
        "    if len(tokens_validos) > 2:\n",
        "        return  \" \".join(tokens_validos)\n",
        "\n",
        "trata_textos(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGmvIa6xZ5Az"
      },
      "source": [
        "textos_para_tratamento = (titulos.lower() for titulos in dataset[\"Paragrafo\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q48GGJ1mZ5A0"
      },
      "source": [
        "t0 = time()\n",
        "textos_tratados = [trata_textos(doc) for doc in nlp.pipe(textos_para_tratamento,\n",
        "                                                        batch_size = 1000,\n",
        "                                                        n_process = -1)]\n",
        "tf = time() - t0\n",
        "tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyuy12ZtZ5A0"
      },
      "source": [
        "### Texto tratados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYKNNZjyZ5A0"
      },
      "source": [
        "titulos_tratados = pd.DataFrame({\"Tratados\": textos_tratados})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwqsL36sZ5A0"
      },
      "source": [
        "titulos_tratados.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-N8BKgXZ5A0"
      },
      "source": [
        "tratado_nlp = nlp(str(textos_tratados))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBhoJMfJZ5A0"
      },
      "source": [
        "tratado_nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AChxndURZ5A1"
      },
      "source": [
        "### Frase mais longa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGcZuxMAZ5A1"
      },
      "source": [
        "alienSentLengths = [len(sent) for sent in tratado_nlp.sents]\n",
        "maiorFrase = [sent for sent in tratado_nlp.sents if len(sent) == max(alienSentLengths)]\n",
        "print(maiorFrase[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcx1Hj6JZ5A1"
      },
      "source": [
        "### Gráficos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yIHHFxuZ5A1"
      },
      "source": [
        "pd.Series([word.i for word in tratado_nlp if word.text == 'dias']).hist(figsize=(12,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dz7kcWjZ5A1"
      },
      "source": [
        "pd.Series([word.i for word in tratado_nlp if word.text == 'hélio']).hist(figsize=(12,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy4QTurPZ5A1"
      },
      "source": [
        "def getIndices(w):\n",
        "  hist, bins = np.histogram(pd.Series([word.i for word in tratado_nlp if word.text == w]), bins=50)\n",
        "  return hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAaNJl6yZ5A1"
      },
      "source": [
        "### Se quisermos analisar a proximidade das palavras que escolhemos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1yGuKgDZ5A2"
      },
      "source": [
        "wordList = ['dias', 'hélio','veículos']\n",
        "characterList = ['alemanha', 'brasil','partido']\n",
        "wordIndices = [getIndices(w) for w in wordList]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVZg0ubGZ5A2"
      },
      "source": [
        "wordsDF = pd.DataFrame(wordIndices, index=wordList).T\n",
        "wordsDF.plot(subplots=True, figsize=(14,8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TekCtgjZ5A2"
      },
      "source": [
        "wordsDF.corr().style.background_gradient(cmap='coolwarm').set_precision(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LHGQb1vZ5A2"
      },
      "source": [
        "def narrativeTime(w, text):\n",
        "  return np.histogram(pd.Series([word.i for word in text \n",
        "           if word.lemma_ == w]), 50)[0]\n",
        "\n",
        "def wordCorrelation(wordlist, text):\n",
        "  df = pd.DataFrame([narrativeTime(w, text)\n",
        "                 for w in wordlist], index=wordlist)\n",
        "  return df.T.corr().style.background_gradient(cmap='coolwarm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm_TGqbfZ5A2"
      },
      "source": [
        "### Aqui analisamos a proximidade de algumas palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cGZQ4pbZ5A2"
      },
      "source": [
        "wordCorrelation(['alemanha', 'ocidental', 'polícia', 'federal'], \n",
        "                tratado_nlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD19mBNKZ5A3"
      },
      "source": [
        "### Busca de entidades"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIeMaODQZ5A3"
      },
      "source": [
        "set([w.label_ for w in tratado_nlp.ents])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-hho96_Z5A3"
      },
      "source": [
        "### Mostrando a frase onde aparece nome de pessoas PER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cict3iCwZ5A3"
      },
      "source": [
        "entSents = [ent.sent for ent in tratado_nlp.ents if ent.label_ == 'PER']\n",
        "entSents[2:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CclhapT5Z5A3"
      },
      "source": [
        "### Destacando a entidade na última frase da lista acima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqWQUehIZ5A3"
      },
      "source": [
        "spacy.displacy.render(entSents[3], style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-5Qzgk4Z5A3"
      },
      "source": [
        "### Análise sintática"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAdxJR-aZ5A4"
      },
      "source": [
        "print([w for w in tratado_nlp if w.pos_ == 'NOUN'][:50])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb3_RnNZZ5A4"
      },
      "source": [
        "## Parsing de Dependências"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yv1Vh6HZ5A4"
      },
      "source": [
        "frase10 = [token for token in tratado_nlp.sents][4]\n",
        "frase10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPpmY225Z5A4"
      },
      "source": [
        "spacy.displacy.render(frase10,style='dep',jupyter=True,options={'distance': 140})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXroWuW_Z5A4"
      },
      "source": [
        "### Navegação na árvore de dependências"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rXQkyKXZ5A4"
      },
      "source": [
        "raiz = frase10.root\n",
        "raiz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC2ffu0IZ5A4"
      },
      "source": [
        "filhos = raiz.children\n",
        "list(filhos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m66m4VGziByc"
      },
      "source": [
        "filho = list(raiz.children)[2]\n",
        "filho"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5yXcEg4Z5A5"
      },
      "source": [
        "### Word Embeddings (Word Vectors)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlxJu5Z9Z5A5"
      },
      "source": [
        "vetores = [word for word in tratado_nlp if word.pos_.startswith('N')][9:16]\n",
        "vetores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkCJLheHh8wg"
      },
      "source": [
        "### Agora, vamos pegar os vetores e seus textos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obyVUdY6Z5A5"
      },
      "source": [
        "word_vector = [word.vector for word in vetores]\n",
        "word_string = [word.string.strip() for word in vetores]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBEC97Wdh2xO"
      },
      "source": [
        "### Observando o vetor, vemos que cada palavra possui um vetor de 96 posições:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWOv7HNegloE"
      },
      "source": [
        "word_vector[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shbHl69yhydI"
      },
      "source": [
        "### Olhando apenas o primeiro vetor, vemos sua representação numérica:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I488w58Kgzp6"
      },
      "source": [
        "word_vector[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAlmIcTLg3v5"
      },
      "source": [
        "word_string[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFa2RnDZhKyY"
      },
      "source": [
        "### Transformado para um par ordenado, temos nosso exemplo agora representado assim:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W5PHkalg6Vc"
      },
      "source": [
        "lsa = TruncatedSVD(n_components=2)\n",
        "lsaOut = lsa.fit_transform(word_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgSPNHbXhUa8"
      },
      "source": [
        "lsaOut"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0r3-qDThXqh"
      },
      "source": [
        "xs, ys = lsaOut[:,0], lsaOut[:,1]\n",
        "for i in range(len(xs)): \n",
        "    plt.scatter(xs[i], ys[i])\n",
        "    plt.annotate(word_string[i], (xs[i], ys[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lexmfSFNhoZS"
      },
      "source": [
        "xs[0], ys[0], word_string[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGsD7Apt5oFs"
      },
      "source": [
        "### Gráfico das nóticias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fs4mNtd52kE"
      },
      "source": [
        "d = path.dirname(file) if \"file\" in locals() else os.getcwd()\n",
        "\n",
        "todas_palavras_tratados = ' '.join([texto for texto in titulos_tratados['Tratados']])\n",
        "logo = np.array(Image.open(path.join(\"/content/sample_data/logo.jpg\")))\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.add(\"said\")\n",
        "\n",
        "nuvem_palavras_tratados = WordCloud(width=1000, height=600, mask=logo,\n",
        "                        max_font_size=110, collocations=False, contour_color='gray',contour_width=3,background_color='white').generate(todas_palavras_tratados)\n",
        "\n",
        "nuvem_palavras_tratados.generate(todas_palavras_tratados)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.imshow(nuvem_palavras_tratados, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x0rhSLVsBkS"
      },
      "source": [
        "### Gráfico de intents Notícias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAIwgBGnrd2J"
      },
      "source": [
        "# transforma em vetor de frases\n",
        "texts_noticia = [sent for sent in tratado_nlp.sents]\n",
        "texts_noticia_10 = texts_noticia[15:20]\n",
        "vectors = []\n",
        "# codifica as palavras das frases em vetores de word embeddings\n",
        "for text in texts_noticia_10:\n",
        "  vectors.append(np.mean([word.vector for word in texts_noticia_10], axis=0))\n",
        "M = np.stack(vectors, axis=0)\n",
        "sampled_idxs_noticia = np.random.choice(range(len(texts_noticia_10)), len(texts_noticia_10), replace=False)\n",
        "texts_vec_noticia = np.array([str(t) for t in texts_noticia_10])[sampled_idxs_noticia]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P3CeHogrxLf"
      },
      "source": [
        "model = TSNE(n_components=2, random_state=0, metric='cosine')\n",
        "X = model.fit_transform(M[sampled_idxs_noticia]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oMjnoKqr6lW"
      },
      "source": [
        "fig = px.scatter(text=texts_vec_noticia,x=X[:, 0], y=X[:, 1],range_x=[-100,350])\n",
        "fig.update_traces(textposition='top center')\n",
        "fig.update_layout(title='Representação de entents')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN62g35OlcHK"
      },
      "source": [
        "# Treinando Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZxxPBvXlbU0"
      },
      "source": [
        "pd.set_option('display.max_column',None)\n",
        "pd.set_option('display.max_rows',None)\n",
        "pd.set_option('display.max_seq_items',None)\n",
        "pd.set_option('display.max_colwidth', 500)\n",
        "pd.set_option('expand_frame_repr', True)\n",
        "\n",
        "df = pd.read_csv('/content/sample_data/Tweets_Mg.csv', encoding='utf-8')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGOGbiYD5JeC"
      },
      "source": [
        "### Usando um DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgJhdmmdziPJ"
      },
      "source": [
        "df_total = pd.DataFrame(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N_OFuXS5TOU"
      },
      "source": [
        "###Criando Tweets e Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwrLoG_UlQOI"
      },
      "source": [
        "tweets = df['Text'][:6000]\n",
        "tweets_df = pd.DataFrame(tweets)\n",
        "\n",
        "classes = df['Classificacao'][:6000]\n",
        "classes_df = pd.DataFrame(classes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBHgZypN5V1L"
      },
      "source": [
        "### Convertendo para NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCAqKcuEntHf"
      },
      "source": [
        "tweets_nlp = nlp(str(tweets))\n",
        "classes_nlp = nlp(str(classes))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HrDgb0xttpp"
      },
      "source": [
        "tweets_nlp.ents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw4hMRg4ttg9"
      },
      "source": [
        "classes_nlp.ents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33s509UKmy__"
      },
      "source": [
        "print(\"Tweets\",len(tweets_nlp),\"palavras, Classes\", len(classes_nlp),\"palavras.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE3VVJvCmy4e"
      },
      "source": [
        "print(\"Tweets Sents\",len(list(tweets_nlp.sents)),\"palavras, Classes Sents\", len(list(classes_nlp.sents)),\"palavras.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34XF2oTXtIF-"
      },
      "source": [
        "def trata_textos(doc):\n",
        "    tokens_validos = []\n",
        "    for token in doc:\n",
        "        e_valido = not token.is_stop and token.is_alpha\n",
        "        if e_valido:\n",
        "            tokens_validos.append(token.text)\n",
        "\n",
        "    if len(tokens_validos) > 2:\n",
        "        return  \" \".join(tokens_validos)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3Q9DlJ0wpYm"
      },
      "source": [
        "textos_para_tratamento_tweets = (titulos.lower() for titulos in tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6myctHsehww"
      },
      "source": [
        "tweets_tratado = trata_textos(tweets_nlp)\n",
        "classes_nlp_tratado = trata_textos(classes_nlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3AavHwvyJez"
      },
      "source": [
        "t0 = time()\n",
        "textos_tratados_tweets= [trata_textos(doc) for doc in nlp.pipe(textos_para_tratamento_tweets,\n",
        "                                                        batch_size = 1000,\n",
        "                                                        n_process = -1)]\n",
        "tf = time() - t0\n",
        "tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8cYaHM6yVkl"
      },
      "source": [
        "titulos_tratados_tweets = pd.DataFrame({\"Testo_Tweets\": textos_tratados_tweets})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAcsMLm25nV8"
      },
      "source": [
        "### Tweets Tratados e Retirando NAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy4eRmHhyYgA"
      },
      "source": [
        "titulos_tratados_tweets = titulos_tratados_tweets.dropna()\n",
        "titulos_tratados_tweets.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ7On-eq6UHk"
      },
      "source": [
        "### Tweets não Tratados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQtyYh0n6Rx4"
      },
      "source": [
        "tweets.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bTqb9G8YjME"
      },
      "source": [
        "### Gráfico de palavra ao longo da narrativa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w26iUEa8dPmz"
      },
      "source": [
        "tweets_nlp = nlp(str(textos_tratados_tweets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcMBaqgdfgK_"
      },
      "source": [
        "### "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23Fyr9d4oa6C"
      },
      "source": [
        "pd.Series([word.i for word in tweets_nlp if word.text == 'governador']).hist(figsize=(12,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28sciGYuYlOe"
      },
      "source": [
        "### Gráfico Negativo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqisERQloa1x"
      },
      "source": [
        "pd.Series([word.i for word in classes_nlp if word.text == 'Negativo']).hist(figsize=(12,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsYxuf8hzLsA"
      },
      "source": [
        "### Gráfico Neutro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8n2HfEBYWmy"
      },
      "source": [
        "pd.Series([word.i for word in classes_nlp if word.text == 'Neutro']).hist(figsize=(12,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTbhReRrYq6p"
      },
      "source": [
        "### Gráfico Positivo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSKpFSTZYaHg"
      },
      "source": [
        "pd.Series([word.i for word in classes_nlp if word.text == 'Positivo']).hist(figsize=(12,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOc3c1z_oazc"
      },
      "source": [
        "def getIndices(w):\n",
        "  hist, bins = np.histogram(pd.Series([word.i for word in tweets_nlp if word.text == w]), bins=50)\n",
        "  return hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FzGYcgSoaw5"
      },
      "source": [
        "wordList = ['catedral', 'valadares','governador','aluno','prefeitura','processo']\n",
        "characterList = ['questionar ', 'aumento ','prefeito ','forminga','morte','casos']\n",
        "wordIndices = [getIndices(w) for w in wordList]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI5wvDSToauN"
      },
      "source": [
        "wordsDF = pd.DataFrame(wordIndices, index=wordList).T\n",
        "wordsDF.plot(subplots=True, figsize=(14,14))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGhqC1f3Zkub"
      },
      "source": [
        "wordsDF = pd.DataFrame(wordIndices, index=characterList).T\n",
        "wordsDF.plot(subplots=True, figsize=(14,14))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmI6MI-YBclQ"
      },
      "source": [
        "### Aproximidades das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-Qrovd8oaqp"
      },
      "source": [
        "wordsDF.corr().style.background_gradient(cmap='coolwarm').set_precision(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-n3zhzfmywi"
      },
      "source": [
        "def narrativeTime(w, text):\n",
        "  return np.histogram(pd.Series([word.i for word in text \n",
        "           if word.lemma_ == w]), 50)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYLjp58QDwe-"
      },
      "source": [
        "def wordCorrelation(wordlist, text):\n",
        "  df = pd.DataFrame([narrativeTime(w, text)\n",
        "                 for w in wordlist], index=wordlist)\n",
        "  return df.T.corr().style.background_gradient(cmap='coolwarm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN7L1iQFyyUv"
      },
      "source": [
        "### Aproximidades das palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmOlsqMMDwdH"
      },
      "source": [
        "wordCorrelation(['catedral', 'valadares','governador','aluno','prefeitura','processo'], \n",
        "                tweets_nlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS9Jf8OuyquH"
      },
      "source": [
        "### Entidades nomeadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7RYe6y_Dwbi"
      },
      "source": [
        "set([w.label_ for w in tweets_nlp.ents])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogMHEMjPyj5l"
      },
      "source": [
        "### Mostrando frases onde há nomes de pessoas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzF1coPzDwZG"
      },
      "source": [
        "entSents = [ent.sent for ent in tweets_nlp.ents if ent.label_ == 'PER']\n",
        "entSents[1:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bku3sRv1yfkr"
      },
      "source": [
        "### Destacando a entidade"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-xtDf9sgOtl"
      },
      "source": [
        "spacy.displacy.render(entSents[1:4], style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCAJLQ7QDwXO"
      },
      "source": [
        "print([w for w in tweets_nlp if w.pos_ == 'NOUN'][:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpMEhY3ryUS5"
      },
      "source": [
        "### Parsing de Dependências"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qdwGq5wDwVu"
      },
      "source": [
        "frase10 = [token for token in tweets_nlp.sents][18]\n",
        "frase10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meSlT3G5x32m"
      },
      "source": [
        "### Relações prováveis entre as palavras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eotVbIqYDwTJ"
      },
      "source": [
        "spacy.displacy.render(frase10,style='dep',jupyter=True,options={'distance': 140})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNWhznNExxkK"
      },
      "source": [
        "### Navegação pela árvore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgG7t55pDwRL"
      },
      "source": [
        "raiz = frase10.root\n",
        "raiz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i31UdWl3DwO1"
      },
      "source": [
        "filhos = raiz.children\n",
        "list(filhos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dJmMsPahJu_"
      },
      "source": [
        "filho = list(raiz.children)[2]\n",
        "filho"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsT9xmgLxrjH"
      },
      "source": [
        "### Vamos analisar os vetores de palavras Tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqHSnoa3DwK5"
      },
      "source": [
        "vetores_tweets = [word for word in tweets_nlp if word.pos_.startswith('N')][6:16]\n",
        "vetores_tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW2BiCt2xbFk"
      },
      "source": [
        "### Vetor com 131100606 ... nº posições:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzIZQk9QxQba"
      },
      "source": [
        "vetores_tweets[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GREuOlEMDwIJ"
      },
      "source": [
        "tweets_vector = [word.vector for word in vetores_tweets]\n",
        "tweets_string = [word.string.strip() for word in vetores_tweets]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2FEIxquDwFN"
      },
      "source": [
        "tweets_vector[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "094CNI1kE8X_"
      },
      "source": [
        "tweets_vector[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klopfpJjE8WV"
      },
      "source": [
        "lsa = TruncatedSVD(n_components=2)\n",
        "lsaOut = lsa.fit_transform(tweets_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91lH7U4Aw6xb"
      },
      "source": [
        "### Transformado para um par ordenado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNv_f5xmE8TO"
      },
      "source": [
        "lsaOut"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBtDsDOxwtPq"
      },
      "source": [
        "### Plotar algumas palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXULoacgE8QT"
      },
      "source": [
        "xs, ys = lsaOut[:,0], lsaOut[:,1]\n",
        "for i in range(len(xs)): \n",
        "    plt.scatter(xs[i], ys[i])\n",
        "    plt.annotate(tweets_string[i], (xs[i], ys[i]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcSr7GFuDv9y"
      },
      "source": [
        "lsaOut"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8So4SyO8FIiS"
      },
      "source": [
        "xs[0], ys[0], tweets_string[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9q9JDEYYr0e"
      },
      "source": [
        "### Nuvem de palavras Twwets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty68lwgKYv1h"
      },
      "source": [
        "d = path.dirname(file) if \"file\" in locals() else os.getcwd()\n",
        "\n",
        "todas_palavras_tweets = ' '.join([texto for texto in titulos_tratados_tweets['Testo_Tweets']])\n",
        "logo = np.array(Image.open(path.join(\"/content/sample_data/logo.jpg\")))\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.add(\"said\")\n",
        "\n",
        "nuvem_palavras_tweets = WordCloud(width=1000, height=600, mask=logo,\n",
        "                        max_font_size=110, collocations=False, contour_color='gray',contour_width=3,background_color='white').generate(todas_palavras_tweets)\n",
        "\n",
        "nuvem_palavras_tweets.generate(todas_palavras_tweets)\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.imshow(nuvem_palavras_tweets, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moC3Yo1Avegt"
      },
      "source": [
        "### Puxa as entidades e apenas armazenas os itens unicos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AqgG4KwkHqL"
      },
      "source": [
        "lista_entstreino = []\n",
        "for token in tweets_nlp.ents:\n",
        "  if token.orth_ not in lista_entstreino:\n",
        "    lista_entstreino.append(token.orth_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiBaIFHaks6w"
      },
      "source": [
        "lista_ents_treino = nlp(str(lista_entstreino))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfVj4usqi_zD"
      },
      "source": [
        "### Analisando Intents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RBJe4YXh6Rk"
      },
      "source": [
        "# transforma em vetor de frases\n",
        "texts = [sent for sent in tweets_nlp.sents]\n",
        "texts_10 = texts[30:38]\n",
        "vectors = []\n",
        "# codifica as palavras das frases em vetores de word embeddings\n",
        "for text in texts_10:\n",
        "  vectors.append(np.mean([word.vector for word in texts_10], axis=0))\n",
        "M = np.stack(vectors, axis=0)\n",
        "sampled_idxs = np.random.choice(range(len(texts_10)), len(texts_10), replace=False)\n",
        "texts_vec = np.array([str(t) for t in texts_10])[sampled_idxs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJgTYIWBir1-"
      },
      "source": [
        "model = TSNE(n_components=2, random_state=0, metric='cosine')\n",
        "X = model.fit_transform(M[sampled_idxs]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5KvZk6dltjb"
      },
      "source": [
        "### Gráfico de intents Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fIbKUZBit9Y"
      },
      "source": [
        "fig = px.scatter(text=texts_vec,x=X[:, 0], y=X[:, 1],range_x=[-100,350])\n",
        "fig.update_traces(textposition='top center')\n",
        "fig.update_layout(title='Representação de entidades')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JlpMknRnfqF"
      },
      "source": [
        "### Treino Tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCkvBgOPmJH9"
      },
      "source": [
        "tweets_valor = df['Text'].values\n",
        "classes_valor = df['Classificacao'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQWubKMnuwuK"
      },
      "source": [
        "### Construindo o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7oRMR5undhT"
      },
      "source": [
        "vectorizer = CountVectorizer(analyzer = \"word\")\n",
        "freq_tweets = vectorizer.fit_transform(tweets_valor)\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "modelo = MultinomialNB()\n",
        "modelo.fit(freq_tweets, classes_valor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JmFaiceurYU"
      },
      "source": [
        "### Testando modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPBI9ACunyhP"
      },
      "source": [
        "tweets_valor = titulos_tratados.Tratados\n",
        "freq_testes = vectorizer.transform(tweets_valor)\n",
        "sentimento_tratado = modelo.predict(freq_testes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlSU512RppMF"
      },
      "source": [
        "titulos_tratados.insert(1,'Sentimentos',sentimento_tratado)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0AFb6PDqPES"
      },
      "source": [
        "### Usando a classificação do Tweets para treinar o Noticias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brq_Y5CvqEHw"
      },
      "source": [
        "titulos_tratados"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIz0u6KxD2PN"
      },
      "source": [
        "### Transformando em uma to disc para grava no Mongo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCwfCTDFAiEz"
      },
      "source": [
        "dicionario = titulos_tratados.to_dict('list')\n",
        "tweets_noticias = ((dicionario))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6HyVutx7gl7"
      },
      "source": [
        "### Gravando Tratados e Sentimento no Mongo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATsN4PWQC2HD"
      },
      "source": [
        "tweets_noticias_valor = tweets_noticias\n",
        "\n",
        "if not gravando.find_one(tweets_noticias_valor):       \n",
        "  gravando_banco = gravando.insert_one(tweets_noticias_valor)\n",
        "  print(f\"Gravado com sucesso!\")\n",
        "else:\n",
        "    print(f'Já Existe')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnYu7xNevTgE"
      },
      "source": [
        "### Avaliando modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsJOyyDmn9hN"
      },
      "source": [
        "resultados = cross_val_predict(modelo, freq_tweets, classes_valor, cv = 10)\n",
        "resultados"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL_mIYRAvFaw"
      },
      "source": [
        "### Acuracia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTQB1hI0oFVH"
      },
      "source": [
        "metrics.accuracy_score(classes_valor, resultados)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLdyOJyDu-oM"
      },
      "source": [
        "### Medidas de validação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4IY9-nzoIOS"
      },
      "source": [
        "sentimentos = [\"Positivo\", \"Negativo\", \"Neutro\"]\n",
        "print(metrics.classification_report(classes_valor, resultados, sentimentos))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sobmRBnmu6CA"
      },
      "source": [
        "### Matriz de confusão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYG3Bsw2oMG7"
      },
      "source": [
        "print(pd.crosstab(classes_valor, resultados, rownames = [\"Real\"], colnames=[\"Predito\"], margins=True))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}