{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd07e279da1b63a46ffd75cf03aef1cd603d07e2c884411c9c94fb0aa82f7f9cf87",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "b2a6f09527545e370935e82da67d9200a6ba8ae5f29d2a729dc4524a539d679f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## *Análise de Sentimentos de Artigos*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Artigos Escolhidos\n",
    "\n",
    "1970: Desníveis e falta de sinalização deixam via Anchieta mais perigosa\n",
    "\n",
    "1970: Parlamento da Itália aprova a lei do divórcio; Vaticano reage\n",
    "\n",
    "1970: Itália discute fazer referendo sobre divórcio após lei ser aprovada\n",
    "\n",
    "1970: Programa de construção da ponte Rio-Niterói começa a ser revisto\n",
    "\n",
    "1970: Missão da ONU diz que Portugal foi responsável por invasão da Guiné\n",
    "\n",
    "1970: Inaugurado, viaduto sobre praça 14 Bis facilita ligação centro-zona sul\n",
    "\n",
    "1970: Chanceler da Alemanha Ocidental chega à Polônia para assinar tratado\n",
    "\n",
    "1970: Morre Abrahão de Moraes, um dos maiores astrônomos brasileiros\n",
    "\n",
    "1970: Rio tem policiamento ostensivo após sequestro de embaixador suíço\n",
    "\n",
    "1970: Governo aguarda por mensagem do embaixador sequestrado no Rio\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações / Imports\n",
    "\n",
    "# Web Scraping\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "\n",
    "# Mongo DB\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# SpaCy Stopword WordCloud\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Análise de Sentimentos\n",
    "# Sentiment Analysis\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "source": [
    "## *WEB SCRAPING*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Armazenando todas as URLs em uma lista para automatizar o processo em um loop\n",
    "Collecting all URLs in a single list to automate the process in a loop\n",
    "'''\n",
    "urls = ['https://www1.folha.uol.com.br/banco-de-dados/2020/11/1970-desniveis-e-falta-de-sinalizacao-deixam-via-anchieta-mais-perigosa.shtml',\n",
    "        'https://www1.folha.uol.com.br/banco-de-dados/2020/12/1970-parlamento-da-italia-aprova-a-lei-do-divorcio-vaticano-reage.shtml',\n",
    "        'https://www1.folha.uol.com.br/banco-de-dados/2020/12/1970-italia-discute-fazer-referendo-sobre-divorcio-apos-lei-ser-aprovada.shtml',\n",
    "        'https://www1.folha.uol.com.br/banco-de-dados/2020/12/1970-programa-de-construcao-da-ponte-rio-niteroi-comeca-a-ser-revisto.shtml',\n",
    "        'https://www1.folha.uol.com.br/banco-de-dados/2020/12/1970-missao-da-onu-diz-que-portugal-foi-responsavel-por-invasao-da-guine.shtml',\n",
    "        'https://www1.folha.uol.com.br/banco-de-dados/2020/12/1970-inaugurado-viaduto-sobre-praca-14-bis-facilita-ligacao-centro-zona-sul.shtml',\n",
    "        'https://www1.folha.uol.com.br/banco-de-dados/2020/12/1970-chanceler-da-alemanha-ocidental-chega-a-polonia-para-assinar-tratado.shtml',\n",
    "        'https://www1.folha.uol.com.br/banco-de-dados/2020/12/1970-morre-abrahao-de-moraes-um-dos-maiores-astronomos-brasileiros.shtml',\n",
    "        'https://www1.folha.uol.com.br/banco-de-dados/2020/12/1970-rio-tem-policiamento-ostensivo-apos-sequestro-de-embaixador-suico.shtml',\n",
    "        'https://www1.folha.uol.com.br/banco-de-dados/2020/12/1970-governo-aguarda-por-mensagem-do-embaixador-sequestrado-no-rio.shtml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "O loop tem a função de juntar todos os paragráfos de cada artigo como um elemento só e agregar todos eles em uma lista, desta forma, temos todos os 10 textos e cada parágrafo em uma lista específica para realizar a análise de sentimentos posteriormente.\n",
    "The loop has the function of join all paragraphs of each article in a single element and aggregate all of them in a list, this way, we have all texts and each paragraph in a specific list to do the sentiment analysis after.\n",
    "'''\n",
    "textos = []\n",
    "paragrafos = []\n",
    "for iterador in range(len(urls)):\n",
    "    r = requests.get(urls[iterador])\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    texto = ''\n",
    "    p_artigo = soup.find('div', class_='c-news__body').findAll('p')\n",
    "    for p in p_artigo:\n",
    "        paragrafos.append(p.text)\n",
    "        texto += ''.join(p.findAll(text = True))\n",
    "    textos.append(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Conferindo se ocorreu como esperava.\n",
    "Checking if occured as expected.\n",
    "'''\n",
    "print(textos)\n",
    "print(f'Tamanho da lista: {len(textos)}')"
   ]
  },
  {
   "source": [
    "## *MONGO DB*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Criando lista com o nome dos artigos para gravar como o nome no banco de dados e o valor sendo o texto do nome correspondente ao artigo.\n",
    "Salvando todos eles no MongoDB com o nome da coleção de 'textos'.\n",
    "Creating a list with the articles's name to store in the database e the value being the text of the corresponding article.\n",
    "Saving all of them in the Mongo DB with the collection's name 'textos'.\n",
    "'''\n",
    "artigos = [\"Desníveis e falta de sinalização deixam via Anchieta mais perigosa\", \"Parlamento da Itália aprova a lei do divórcio; Vaticano reage\", \"Itália discute fazer referendo sobre divórcio após lei ser aprovada\", \"Programa de construção da ponte Rio-Niterói começa a ser revisto\",\n",
    "\"Missão da ONU diz que Portugal foi responsável por invasão da Guiné\", \"Inaugurado, viaduto sobre praça 14 Bis facilita ligação centro-zona sul\",\n",
    "\"Chanceler da Alemanha Ocidental chega à Polônia para assinar tratado\", \"Morre Abrahão de Moraes, um dos maiores astrônomos brasileiros\",\n",
    "\"Rio tem policiamento ostensivo após sequestro de embaixador suíço\", \"Governo aguarda por mensagem do embaixador sequestrado no Rio\"]\n",
    "\n",
    "dict_textos = dict()\n",
    "\n",
    "for i in range(len(textos)):\n",
    "    dict_textos[artigos[i]] = textos[i]\n",
    "\n",
    "client = MongoClient()\n",
    "name = 'textos'\n",
    "db = client[name]\n",
    "collection = db[name]\n",
    "collection.insert_one(dict_textos)\n"
   ]
  },
  {
   "source": [
    "## *STOPWORDS, NUVEM DE PALAVRAS, VERBOS E ENTIDADES*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gerando WordCloud para cada texto.\n",
    "Transforma cada item da lista 'textos' em uma string, passa para o doc, remove as stopwords e cria o Word Cloud.\n",
    "Generating WordCloud for each text.\n",
    "Transform each list item into a string, moves to doc, remove the stopwords e create the Word Cloud.\n",
    "'''\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "for i in range(len(textos)):\n",
    "    texto = str(textos[i])\n",
    "    doc = nlp(texto)\n",
    "    tokens = [token for token in doc if not token.is_stop and token.is_punct != True]\n",
    "    str_tokens = ' '.join([str(item) for item in tokens])\n",
    "    wordcloud = WordCloud(width=800, height=600, background_color='black', min_font_size=10).generate(str_tokens)\n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Coletando os verbos de todos textos, criando um dicionário relacionando o artigo com seus respectivos verbos e inserindo-os no Mongo DB.\n",
    "Collecting the verbs of all texts, creating a dictionary relating the article with its respective verbs and storing in Mongo DB.\n",
    "'''\n",
    "verbos_list = []\n",
    "for i in range(len(textos)):\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")\n",
    "    texto = str(textos[i])\n",
    "    doc = nlp(texto)\n",
    "    verbos = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
    "    verbos_list.append(verbos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_verbos = dict()\n",
    "\n",
    "for i in range(len(textos)):\n",
    "    dict_verbos[artigos[i]] = verbos_list[i][:]\n",
    "\n",
    "collection.insert_one(dict_verbos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Coletando as entidades de todos textos, criando um dicionário relacionando o artigo com suas respectivas entidades e inserindo-as no Mongo DB.\n",
    "Collecting the entities of all texts, creating a dictionary relating the article with its respective entities and storing in Mongo DB.\n",
    "'''\n",
    "entidades_list = []\n",
    "for i in range(len(textos)):\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")\n",
    "    texto = str(textos[i])\n",
    "    doc = nlp(texto)\n",
    "    entidades = doc.ents\n",
    "    entidades_list.append(str(entidades))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_entidades = dict()\n",
    "\n",
    "for i in range(len(textos)):\n",
    "    dict_entidades[artigos[i]] = entidades_list[i][:]\n",
    "\n",
    "collection.insert_one(dict_entidades)"
   ]
  },
  {
   "source": [
    "## *ANÁLISE DE SENTIMENTOS*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Carregando o arquivo CSV.\n",
    "Loading CSV File\n",
    "'''\n",
    "df = pd.read_csv('Tweets_Mg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Selecionando dados. Os únicos relevantes são o texto e a classificação.\n",
    "Selecting data. The only relevant are text and classification.\n",
    "'''\n",
    "texto_tt = df.Text.values\n",
    "classificacao = df.Classificacao.values\n",
    "\n",
    "'''\n",
    "Treinando o modelo baseado nos tweets.\n",
    "Training the model based on tweets.\n",
    "'''\n",
    "vector = CountVectorizer()\n",
    "f_twt = vector.fit_transform(texto_tt)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(f_twt, classificacao)\n",
    "\n",
    "'''\n",
    "Testando o modelo.\n",
    "Testing the model.\n",
    "'''\n",
    "teste_textos = vector.transform(textos)\n",
    "preditos_textos = model.predict(teste_textos) # Todos deram como resultado 'neutro'.\n",
    "\n",
    "teste_paragrafos = vector.transform(paragrafos)\n",
    "preditos_paragrafos = model.predict(teste_paragrafos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Avaliando o modelo.\n",
    "Evaluating the model.\n",
    "'''\n",
    "resultados = cross_val_predict(model, f_twt, classificacao, cv = 10)\n",
    "score = metrics.accuracy_score(classificacao, resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inserindo os valores preditos pelo modelo e seu score no Mongo DB.\n",
    "Storing the predicted values by model and its score in Mongo DB.\n",
    "'''\n",
    "dict_preditos_score_textos = dict()\n",
    "dict_preditos_score_textos['Preditos'] = list(preditos_textos)\n",
    "dict_preditos_score_textos['Score'] = score\n",
    "\n",
    "dict_preditos_paragrafos = dict()\n",
    "dict_preditos_paragrafos['Preditos Paragrafos'] = list(preditos_paragrafos)\n",
    "\n",
    "collection.insert_one(dict_preditos_score_textos)\n",
    "collection.insert_one(dict_preditos_paragrafos)\n",
    "\n",
    "print(pd.crosstab(classificacao, resultados, rownames = [\"Real\"], colnames=[\"Predito\"], margins=True))"
   ]
  },
  {
   "source": [
    "Autor: Victor Balbino Araujo | Avaliação Compasso"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}